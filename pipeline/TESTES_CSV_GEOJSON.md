# üß™ Testes e Valida√ß√µes - CSV/GeoJSON Integration

## Resumo de Testes

### ‚úÖ Dados Dispon√≠veis

```
üìÇ pipeline/data/
‚îú‚îÄ citizens_sample.csv           (50 cidad√£os, CSV com lat/lon)
‚îú‚îÄ flooding_areas.geojson        (3 √°reas de enchente, GeoJSON Polygon)
‚îî‚îÄ citizens_additional.geojson   (5 cidad√£os extra, GeoJSON Point)
```

### üìä Estat√≠sticas

| Arquivo | Formato | Registros | Geometria | CRS |
|---------|---------|-----------|-----------|-----|
| citizens_sample.csv | CSV | 50 | Point (lat/lon) | EPSG:4326 |
| flooding_areas.geojson | GeoJSON | 3 | Polygon | EPSG:4326 |
| citizens_additional.geojson | GeoJSON | 5 | Point | EPSG:4326 |
| **TOTAL** | **Misto** | **58** | **8 geometrias** | **EPSG:4326** |

---

## üß™ Teste 1: Valida√ß√£o de Dados de Entrada

### CSV Validation (citizens_sample.csv)

```bash
# Verificar estrutura
head -5 pipeline/data/citizens_sample.csv

# Esperado:
# citizen_id,name,age,document_number,latitude,longitude,registered_date,city,district
# C001,Jo√£o Silva,32,12345678901,-30.0326,-51.2093,2024-01-15,Porto Alegre,Centro
# C002,Maria Santos,28,23456789012,-30.0450,-51.3050,2024-01-20,Porto Alegre,Partenon
# ...50 registros
```

**Valida√ß√µes:**
- ‚úì 9 colunas presentes
- ‚úì 50 registros de dados (sem header)
- ‚úì Colunas latitude/longitude parecem v√°lidas
- ‚úì Datas em formato ISO (YYYY-MM-DD)

### GeoJSON Validation (flooding_areas.geojson)

```bash
# Verificar estrutura
cat pipeline/data/flooding_areas.geojson | python -m json.tool

# Esperado:
# {
#   "type": "FeatureCollection",
#   "features": [
#     {
#       "type": "Feature",
#       "properties": {...},
#       "geometry": {
#         "type": "Polygon",
#         "coordinates": [[[lon, lat], ...]]
#       }
#     }
#   ]
# }
```

**Valida√ß√µes:**
- ‚úì V√°lido JSON format
- ‚úì type = "FeatureCollection"
- ‚úì 3 features
- ‚úì geometry.type = "Polygon"
- ‚úì coordinates em formato [longitude, latitude]

---

## üß™ Teste 2: Convers√£o CSV ‚Üí GeoParquet

### Python Test

```python
import geopandas as gpd
import pandas as pd
from pathlib import Path

# Testar leitura de CSV
csv_file = 'pipeline/data/citizens_sample.csv'
df = pd.read_csv(csv_file)

print(f"‚úì CSV lido: {len(df)} registros")
print(f"‚úì Colunas: {df.columns.tolist()}")
print(f"‚úì Tipos:\n{df.dtypes}")

# Testar convers√£o para GeoDataFrame
geometry = gpd.points_from_xy(df['longitude'], df['latitude'])
gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')

print(f"‚úì GeoDataFrame criado: {len(gdf)} geometrias")
print(f"‚úì Geometrias v√°lidas: {gdf.geometry.is_valid.all()}")
print(f"‚úì Bounds: {gdf.total_bounds}")

# Salvar em GeoParquet
output = 'pipeline/etl/bronze/citizens_sample.parquet'
Path(output).parent.mkdir(parents=True, exist_ok=True)
gdf.to_parquet(output)
print(f"‚úì GeoParquet salvo: {output}")
```

### Comando Shell

```bash
# Via Python
python -c "
import geopandas as gpd
import pandas as pd

csv_file = 'pipeline/data/citizens_sample.csv'
df = pd.read_csv(csv_file)
geometry = gpd.points_from_xy(df['longitude'], df['latitude'])
gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')
gdf.to_parquet('pipeline/etl/bronze/citizens_sample.parquet')
print(f'‚úì Convertido: {len(gdf)} registros')
"
```

---

## üß™ Teste 3: Convers√£o GeoJSON ‚Üí GeoParquet

### Python Test

```python
import geopandas as gpd

# Testar leitura de GeoJSON
geojson_file = 'pipeline/data/flooding_areas.geojson'
gdf = gpd.read_file(geojson_file)

print(f"‚úì GeoJSON lido: {len(gdf)} registros")
print(f"‚úì Colunas: {gdf.columns.tolist()}")
print(f"‚úì Geometrias v√°lidas: {gdf.geometry.is_valid.all()}")
print(f"‚úì Tipo de geometria: {gdf.geometry.type.unique()}")

# Salvar em GeoParquet
output = 'pipeline/etl/bronze/flooding_areas.parquet'
gdf.to_parquet(output)
print(f"‚úì GeoParquet salvo: {output}")
```

---

## üß™ Teste 4: Normaliza√ß√£o Silver Layer

### Python Test - CSV

```python
from etl.silver.csv_geojson_converter import CSVGeoJSONToGeoParquetConverter

converter = CSVGeoJSONToGeoParquetConverter()

# Processar arquivo CSV
gdf = converter.process_csv_file(
    'pipeline/data/citizens_sample.csv',
    'pipeline/etl/silver/citizens_sample.parquet'
)

print(f"‚úì CSV normalizado: {len(gdf)} registros")
print(f"‚úì Colunas adicionadas: {[c for c in gdf.columns if c.startswith('_')]}")
print(f"‚úì Arquivo GeoParquet salvo")

# Verificar dados normalizados
print(f"\nDados de exemplo:")
print(gdf.head())
```

### Python Test - GeoJSON

```python
from etl.silver.csv_geojson_converter import CSVGeoJSONToGeoParquetConverter

converter = CSVGeoJSONToGeoParquetConverter()

# Processar arquivo GeoJSON
gdf = converter.process_geojson_file(
    'pipeline/data/flooding_areas.geojson',
    'pipeline/etl/silver/flooding_areas.parquet'
)

print(f"‚úì GeoJSON normalizado: {len(gdf)} registros")
print(f"‚úì Propriedades preservadas: {[c for c in gdf.columns if c != 'geometry']}")
print(f"‚úì Arquivo GeoParquet salvo")

print(f"\nDados de exemplo:")
print(gdf.head())
```

### Teste Autom√°tico Completo

```bash
# Via pipeline
cd pipeline
python -c "
from etl.silver.csv_geojson_converter import CSVGeoJSONToGeoParquetConverter

converter = CSVGeoJSONToGeoParquetConverter()
results = converter.process_all_files()

print(f'‚úì Total de arquivos: {results[\"total_files\"]}')
print(f'‚úì Sucesso: {results[\"successful\"]}')
print(f'‚úì Registros processados: {results[\"total_records\"]}')

for filename, result in results['details'].items():
    if result['status'] == 'success':
        print(f'  ‚úì {filename}: {result[\"records\"]} registros')
    else:
        print(f'  ‚úó {filename}: {result[\"error\"]}')
"
```

---

## üß™ Teste 5: Spatial Join (Gold Layer)

### Python Test

```python
import geopandas as gpd

# Carregar dados normalizados da Silver
citizens = gpd.read_parquet('pipeline/etl/silver/citizens_*.parquet')
flooding_areas = gpd.read_parquet('pipeline/etl/silver/flooding_areas.parquet')

print(f"‚úì Carregados: {len(citizens)} cidad√£os + {len(flooding_areas)} √°reas")

# Spatial Join
affected = gpd.sjoin(citizens, flooding_areas, how='inner', predicate='within')

print(f"‚úì ST_Contains: {len(affected)} cidad√£os dentro de √°reas de enchente")

# N√£o afetados
all_citizen_ids = set(citizens['citizen_id'].unique())
affected_ids = set(affected['citizen_id'].unique())
unaffected_ids = all_citizen_ids - affected_ids

print(f"‚úì Cidad√£os seguros: {len(unaffected_ids)}")
print(f"‚úì Total: {len(all_citizen_ids)}")
```

---

## üß™ Teste 6: Pipeline Completo

### Executar via Docker

```bash
# Setup (primeira vez)
./setup.sh

# Executar pipeline completo
./docker.sh pipeline

# Esperado:
# [1/6] BRONZE - Carregando dados...
# ‚úì Bronze: 3 √°reas + 100 cidad√£os
#
# [2/6] SILVER - Convertendo CSV/GeoJSON ‚Üí GeoParquet...
# ‚úì Convers√£o: 3 arquivo(s) processado(s)
#   Total de registros convertidos: 58
#
# [2b/6] SILVER - Normalizando dados gerados...
# ‚úì Silver: 3 √°reas + 100 cidad√£os
#
# [3/6] GOLD - Batimento geogr√°fico...
# ‚úì Gold: 60+ afetados + 40- n√£o afetados
#
# [4/6] POSTGIS - Importar dados...
# ‚úì PostGIS carregado
#
# [5/6] RESUMO FINAL
# ‚úì PIPELINE CONCLU√çDO COM SUCESSO!
```

### Executar via CLI

```bash
# Setup
pip install geopandas geoparquet pandas shapely boto3 psycopg2

# Run
cd pipeline
python main.py

# Esperado: Sa√≠da id√™ntica
```

### Executar apenas Convers√£o

```bash
cd pipeline
python -m etl.silver.csv_geojson_converter

# Esperado:
# ================================================================================
# INICIANDO CONVERS√ÉO CSV/GeoJSON ‚Üí GeoParquet
# ================================================================================
# Lendo CSV: citizens_sample.csv
# Normalizando CSV...
# Salvando GeoParquet...
# ‚úì citizens_sample.csv: 50 registros ‚Üí etl/silver/citizens_sample.parquet
# 
# Lendo GeoJSON: flooding_areas.geojson
# Normalizando GeoJSON...
# Salvando GeoParquet...
# ‚úì flooding_areas.geojson: 3 registros ‚Üí etl/silver/flooding_areas.parquet
# 
# Lendo GeoJSON: citizens_additional.geojson
# Normalizando GeoJSON...
# Salvando GeoParquet...
# ‚úì citizens_additional.geojson: 5 registros ‚Üí etl/silver/citizens_additional.parquet
# ================================================================================
# RESUMO DA CONVERS√ÉO
# ================================================================================
# Total de arquivos: 3
# Sucesso: 3
# Falhas: 0
# Total de registros: 58
# ================================================================================
```

---

## üß™ Teste 7: Valida√ß√£o de Arquivos Criados

### Listar Arquivos Criados

```bash
# Bronze layer
ls -lh pipeline/etl/bronze/

# Esperado:
# citizens_sample.parquet (50 registros)
# flooding_areas.parquet (3 registros)
# citizens_additional.parquet (5 registros)

# Silver layer
ls -lh pipeline/etl/silver/

# Esperado:
# citizens_sample.parquet (50 normalizados + metadados)
# flooding_areas.parquet (3 normalizados + metadados)
# citizens_additional.parquet (5 normalizados + metadados)
```

### Inspecionar Parquet

```bash
# Via Python
python -c "
import geopandas as gpd

# Bronze
print('=== BRONZE ===')
gdf = gpd.read_parquet('pipeline/etl/bronze/citizens_sample.parquet')
print(f'Registros: {len(gdf)}')
print(f'Colunas: {gdf.columns.tolist()}')
print(f'Geometrias: {gdf.geometry.type.unique()}')
print(gdf.head())

# Silver
print('\n=== SILVER ===')
gdf = gpd.read_parquet('pipeline/etl/silver/citizens_sample.parquet')
print(f'Registros: {len(gdf)}')
print(f'Colunas: {gdf.columns.tolist()}')
print(f'Metadados adicionados: {[c for c in gdf.columns if c.startswith(\"_\")]}')
print(gdf.head())
"
```

---

## üìã Checklist de Testes

- [ ] **Teste 1**: CSV com 50 registros lido corretamente
- [ ] **Teste 2**: GeoJSON com 3 features lido corretamente
- [ ] **Teste 3**: CSV convertido para GeoParquet com geometrias Point
- [ ] **Teste 4**: GeoJSON convertido para GeoParquet com geometrias Polygon
- [ ] **Teste 5**: Normaliza√ß√£o adiciona metadados (_processed_date, _source_type)
- [ ] **Teste 6**: Spatial join encontra cidad√£os em √°reas de enchente
- [ ] **Teste 7**: Pipeline completo executa sem erros
- [ ] **Teste 8**: Dados carregados em PostGIS com sucesso
- [ ] **Teste 9**: Flask Dashboard exibe dados corretamente

---

## üéØ Pr√≥ximas Etapas

1. **Adicionar mais fontes de dados**:
   - Outros distritos de Porto Alegre
   - Dados hist√≥ricos de enchentes
   - Dados demogr√°ficos adicionais

2. **Melhorar qualidade**:
   - Valida√ß√£o mais rigorosa em Silver
   - Deduplica√ß√£o de cidad√£os
   - Padroniza√ß√£o de endere√ßos

3. **Expandir an√°lises**:
   - C√°lculo de dist√¢ncia at√© √°reas de enchente
   - Identifica√ß√£o de rotas de evacua√ß√£o
   - An√°lise de vulnerabilidade

4. **Integrar com sistemas**:
   - API REST para consultas
   - Webhooks para alertas
   - Integra√ß√£o com sistemas de defesa civil
